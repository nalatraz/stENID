{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ed1896",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b447e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ENID.acquisition import *\n",
    "from ENID.interpolation import *\n",
    "from alerce.core import Alerce\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import george\n",
    "import random\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fc922",
   "metadata": {},
   "source": [
    "### Import Source Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e18734",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = pd.read_csv('sources.csv',header=None) # Give your sources in a .csv file with the ZTF-names. An example with blue continuum sources is attached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1377c",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16614d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wanted\n",
    "ModelWeights = 'Models/Simple GRU/Weights'\n",
    "\n",
    "# Name of the saved files\n",
    "Version = strftime(\"%Y%m%d%H%M%S\",gmtime()) #Default used UTC time and date.\n",
    "\n",
    "# Number of points before\n",
    "timediff = 14\n",
    "noofpoint = 2\n",
    "\n",
    "# Plotting?\n",
    "plotbool = 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bde6a",
   "metadata": {},
   "source": [
    "### Download Lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da1a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sources : 55 \n",
      "\n",
      "Found  55 objects\n",
      "Missing  0 objects\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19abzwbxy\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abtuzsb\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abklbam\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abmdpwe\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abqbuaj\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abqcsdq\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abthaii\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18ablwafp\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abvzdvj\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18abwkrbl\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18acurqaw\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18acbxskh\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18acrwheu\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18acszayr\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18adaykvg\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18adazblo\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aamowaf\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aakjcxs\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF18achzddr\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aalubxw\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aajwogx\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aamrais\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aasekcx\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aawethv\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aaydtur\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aavndxo\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19aaykqsk\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19abrtjsf\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19abuilka\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19acblhej\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19acgfvvi\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19ackbclh\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19acvmcdd\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19acyjzeo\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20aaavvlv\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF19abcilax\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20aclkdlu\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20acmmfau\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20acohkja\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20acordmt\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20acpvbbh\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20acoawtj\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20acvjagm\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21aabjfbc\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21aaevrjl\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21aaglrzc\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21aatpsky\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF20abrbeie\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21acadrzr\u001b[0m\n",
      "Warning : Not enought detections. Ignoring entry.\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21acbfuwk\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21aceqmyz\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21aceqpgp\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21abzbwfo\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21acdtscx\u001b[0m\n",
      "\n",
      "Importing lightcurve and metadata for  \u001b[1mZTF21acgyywa\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##### Retrieve light curves part\n",
    "\n",
    "# Get ready by making it a list anc checking that they are not floats\n",
    "ztf_raw = list(datasources[0])\n",
    "ztf_names = [x for x in ztf_raw if type(x) != float]\n",
    "\n",
    "# Find the sources on ALeRCE\n",
    "print('Number of sources :', len(ztf_raw), '\\n')\n",
    "alerce_found, alerce_missing = source_search_alerce(ztf_names)\n",
    "\n",
    "# Tell how many were found\n",
    "print('Found ', alerce_found.shape[0], 'objects')\n",
    "print('Missing ', len(alerce_missing), 'objects')\n",
    "\n",
    "if len(alerce_missing) > 0:\n",
    "    for mis in alerce_missing:\n",
    "        print('Missing ', alerce_missing['oid'][mis])\n",
    "\n",
    "# Get the object id for each source\n",
    "sources = list(alerce_found['oid'])\n",
    "\n",
    "# Retrieve the lightcurves for the sources\n",
    "object_dictionary = {\"Name\": [], \"Data\": [], \"Label\": []}\n",
    "\n",
    "for i in range(len(sources)):\n",
    "    \n",
    "    \n",
    "    lightcurve = lc_compile(sources[i])\n",
    "    if len(lightcurve['R_mag']) > 1 and len(lightcurve['G_mag']) > 1:\n",
    "        object_dictionary['Data'].append(lightcurve)\n",
    "        object_dictionary['Name'].append(ztf_names[i])\n",
    "        object_dictionary['Label'].append(['SN II', 1])\n",
    "    else:\n",
    "        print('Warning : Not enought detections. Ignoring entry.')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "save_file = open(\"pickle_lightcurves.pickle\", mode='wb')\n",
    "pickle.dump(object_dictionary, save_file)\n",
    "save_file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58410e16",
   "metadata": {},
   "source": [
    "### Interpolate Lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d81b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Arrays...\n"
     ]
    }
   ],
   "source": [
    "file = open('pickle_lightcurves.pickle', \"rb\")\n",
    "datadict = pickle.load(file)\n",
    "dummy_dict, source_idx = initialise(datadict,timediff,noofpoint)\n",
    "\n",
    "processed_data = preprocessing('pickle_lightcurves.pickle',Version,timediff,noofpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba209427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dimensions : (54, 2, 200)\n",
      "Label Dimensions : (54, 2)\n"
     ]
    }
   ],
   "source": [
    "datafile = 'Data/'+Version+'/data_lc.npy'\n",
    "labelsfile = 'Data/'+Version+'/labels.npy'\n",
    "\n",
    "data = np.load(datafile)\n",
    "labels = np.load(labelsfile)\n",
    "\n",
    "print('Data Dimensions :', data.shape)\n",
    "print('Label Dimensions :', labels.shape)\n",
    "\n",
    "x = torch.permute(torch.tensor(data), [0,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a257a4",
   "metadata": {},
   "source": [
    "### Build Model & Import Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed9ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_ENID(\n",
      "  (GRU): GRU(2, 64, batch_first=True)\n",
      "  (Dense): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (predict): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class simple_ENID(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(simple_ENID, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, bidirectional=False) \n",
    "        self.Dense = nn.Linear(hidden_dim, output_dim)\n",
    "        self.predict = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_, h_ = self.GRU(x)\n",
    "        x_ = self.Dense(x_[:,-1])\n",
    "        x_ = self.predict(x_)\n",
    "        \n",
    "        return x_\n",
    "    \n",
    "# Model Building\n",
    "input_dim = x.shape[2]\n",
    "num_classes = 6\n",
    "hidden_dim = 64\n",
    "n_layers = 1\n",
    "\n",
    "Net = simple_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "Net.load_state_dict(torch.load(ModelWeights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df340e90",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7172bcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: CV  -  Confidence: 0.9999999\n",
      "Class: CV  -  Confidence: 0.9999999\n",
      "Class: SN Ib/c  -  Confidence: 0.92862153\n",
      "Class: SN II  -  Confidence: 0.8502242\n",
      "Class: SN Ia  -  Confidence: 0.66323805\n",
      "Class: CV  -  Confidence: 0.9999902\n",
      "Class: SN Ib/c  -  Confidence: 0.99895775\n",
      "Class: SN IIn  -  Confidence: 0.979807\n",
      "Class: SN Ib/c  -  Confidence: 0.8655839\n",
      "Class: SN Ib/c  -  Confidence: 0.8580053\n",
      "Class: SN IIn  -  Confidence: 0.9735744\n",
      "Class: SN II  -  Confidence: 0.97399694\n",
      "Class: SN II  -  Confidence: 0.9437273\n",
      "Class: SN Ia  -  Confidence: 0.9890342\n",
      "Class: SN Ia  -  Confidence: 0.9656921\n",
      "Class: SN IIn  -  Confidence: 0.99991703\n",
      "Class: SN II  -  Confidence: 0.999818\n",
      "Class: SN Ib/c  -  Confidence: 0.8886919\n",
      "Class: SN Ib/c  -  Confidence: 0.9753405\n",
      "Class: SN II  -  Confidence: 0.99945694\n",
      "Class: SN II  -  Confidence: 1.0\n",
      "Class: SN IIn  -  Confidence: 0.97051716\n",
      "Class: SN IIn  -  Confidence: 0.99580634\n",
      "Class: SN Ib/c  -  Confidence: 0.76370376\n",
      "Class: SN II  -  Confidence: 0.999915\n",
      "Class: SN II  -  Confidence: 1.0\n",
      "Class: SN Ia  -  Confidence: 0.5267531\n",
      "Class: SN Ia  -  Confidence: 0.9477517\n",
      "Class: SN II  -  Confidence: 0.99999714\n",
      "Class: SLSN-I  -  Confidence: 0.9938651\n",
      "Class: SN Ia  -  Confidence: 0.9999975\n",
      "Class: SN Ia  -  Confidence: 0.9375864\n",
      "Class: SN Ib/c  -  Confidence: 0.8890095\n",
      "Class: SN Ia  -  Confidence: 0.9611757\n",
      "Class: SN Ia  -  Confidence: 0.99978644\n",
      "Class: CV  -  Confidence: 0.99999547\n",
      "Class: SN Ia  -  Confidence: 0.9927084\n",
      "Class: SN Ia  -  Confidence: 0.61854094\n",
      "Class: CV  -  Confidence: 0.9999981\n",
      "Class: SN Ia  -  Confidence: 0.9887533\n",
      "Class: SN Ib/c  -  Confidence: 0.9981173\n",
      "Class: SN IIn  -  Confidence: 0.99944156\n",
      "Class: SN II  -  Confidence: 0.99997485\n",
      "Class: SN Ia  -  Confidence: 0.98851997\n",
      "Class: SN II  -  Confidence: 1.0\n",
      "Class: SN Ia  -  Confidence: 0.44300425\n",
      "Class: SN Ib/c  -  Confidence: 0.989714\n",
      "Class: CV  -  Confidence: 0.9978916\n",
      "Class: SN Ib/c  -  Confidence: 0.5569325\n",
      "Class: SN II  -  Confidence: 0.9992083\n",
      "Class: SN II  -  Confidence: 0.9999809\n",
      "Class: SN IIn  -  Confidence: 0.99874973\n",
      "Class: SN II  -  Confidence: 0.9878197\n",
      "Class: SN IIn  -  Confidence: 0.87422067\n"
     ]
    }
   ],
   "source": [
    "classes = ['CV', 'SN II', 'SN Ia', 'SLSN-I', 'SN IIn', 'SN Ib/c']\n",
    "\n",
    "pred = Net(x.float())\n",
    "\n",
    "label = np.argmax(pred.detach().numpy(), axis = 1)\n",
    "conf = np.max(pred.detach().numpy(), axis = 1)\n",
    "\n",
    "for i in range(len(label)):\n",
    "    print('Class:', classes[label[i]], ' -  Confidence:', conf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2d80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
