{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b334162",
   "metadata": {},
   "source": [
    "## Considerations for Classification\n",
    "Here we have written down some fundamental considerations that have to be made for our given classification problem. \n",
    "\n",
    "#### Small, imbalanced dataset\n",
    "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html\n",
    "- The dataset is quite small, which means that we cannot allow ourselves to split it up in multiple pieces for training, testing and validation. We will consider a 0.9/0.1 proportion of split of training and validation set. Within the 90% of training, we will use k-fold cross-validation to see how the model performs on unseen data before feeding it the validation set (the truly unseen data).\n",
    "- It is imbalanced. \n",
    "- Performance Metric : consider another peformance metric than accuracy (F1 score, for example) and confusion matrices to manually investigate the outputs and see if the model truly learned something, or if it is simply reflecting the underlying distribution of data (i.e. the class imbalance just makes it predict the most common class to achieve a good result, which we do not want). \n",
    "- Resampling data : We can oversample or undersample our data (add or remove samples from respectively the smaller and larger classes) to achieve a better balance. Because we have little data in our case, oversampling is probably the best option. It may lead to over-fitting, though. If we use over-sampling, it should be carried out as a step AFTER cross-validation partitioning, to avoid over-fitting as much as possible.\n",
    "- Penalised model : we can penalise the model more if it misclassifies the minority classes. It should however be inverstigated whether this is possible for a problem with many minority classes. This should be added in the cost function estimates, but we need to find out how to do this for a PyTorch neural network. \n",
    "\n",
    "#### Current Pipeline : \n",
    "1. Pull lightcurves for ZTF sources from Alerce, get classes from TNS\n",
    "2. Interpolate the data and merge all lightcurves in a single array\n",
    "3. One-hot encode labels\n",
    "4. Create training and test sets (proportions 90%/10%)\n",
    "5. Use K-fold cross validation for model training\n",
    "6. Over-sample the minority classes in each fold from the training set\n",
    "\n",
    "On K-fold cross-validation implementation in PyTorch : \n",
    "\n",
    "https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/#summary-and-code-example-k-fold-cross-validation-with-pytorch\n",
    "\n",
    "#### Input to RNN\n",
    "- The RNN has no information about the time, so we need to feed it an array containing the lightcurve magnitude data points for each source. Ideally, the \"timestep\" (that it does not know) should be the same between each point, so that the RNN sees a data point for a source every X minutes/hours/days in all cases. This is an important pre-processing step for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eeca16",
   "metadata": {},
   "source": [
    "## Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4660724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ENID.interpolation import label_encoding\n",
    "from ENID.DOClassify import network_train, network_evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84d7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_train(network, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory):\n",
    "    \n",
    "    # Initialisation\n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "    \n",
    "    epoch_loss = []\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs+1):\n",
    "\n",
    "        permutation = torch.randperm(X_train.size()[0])\n",
    "        batch_loss = 0\n",
    "        count = 0\n",
    "\n",
    "        for i in range(0,X_train.size()[0], batch_size):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = permutation[i:i+batch_size]\n",
    "\n",
    "            batch_x, batch_y = X_train[indices], Y_train[indices]\n",
    "\n",
    "            outputs = network(batch_x.float())\n",
    "\n",
    "            loss = evaluate(outputs, batch_y.float())\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss.append(batch_loss/count)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, batch_loss/count))\n",
    "        elif epoch == 69:\n",
    "            print(\"Epoch: %d, Nice, loss: %1.5f\" % (epoch, batch_loss/count))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_loss, linewidth=3)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross-Entropy Loss')\n",
    "    plt.title('Loss Evolution During Training')\n",
    "\n",
    "    plt.savefig(version_directory+'/TrainingLoss.png')\n",
    "    \n",
    "    return network, min(epoch_loss)\n",
    "\n",
    "def network_evaluation(network, X_test, Y_test, train_loss, evaluate, version_directory):\n",
    "    \n",
    "    torch.save(network.state_dict(), version_directory+'/Weights')\n",
    "\n",
    "    outputs = network(X_test.float())\n",
    "\n",
    "    loss = evaluate(outputs, Y_test).item()\n",
    "    y_pred = np.argmax(outputs.detach().numpy(), axis = 1)\n",
    "    y_true = np.argmax(Y_test.detach().numpy(), axis = 1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    print('Train Loss : ' + str(train_loss))\n",
    "    print('Test Loss : ' + str(loss))\n",
    "    print('Global Accuracy : ' + str(round(accuracy*100,2)) + '%')\n",
    "    print('F1 Measure : ' + str(round(f1,2)))\n",
    "\n",
    "    df = pd.DataFrame({'Metrics': ['Train Loss', 'Test Loss', 'F1 Measure', 'Global Accuracy'], 'Values': np.array([train_loss, loss, round(accuracy*100,2), round(f1,2)])})\n",
    "    df.to_csv(version_directory+'/Metrics')\n",
    "    \n",
    "    C = np.round(C, 2)\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels', fontsize=20);\n",
    "    ax.set_ylabel('True labels', fontsize=20); \n",
    "    ax.set_title('Confusion Matrix for Predictions on Test Data', fontsize=25); \n",
    "    ax.xaxis.set_ticklabels(['CV', 'SN II', 'SN Ia', 'SLSN-I', 'SN IIn', 'SN Ib/c'], fontsize=15); \n",
    "    ax.yaxis.set_ticklabels(['CV', 'SN II', 'SN Ia', 'SLSN-I', 'SN IIn', 'SN Ib/c'], fontsize=15);\n",
    "\n",
    "    plt.savefig(version_directory+'/ConfusionMatrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42dd498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dimensions : (4428, 2, 200)\n",
      "Label Dimensions : (4428, 2)\n"
     ]
    }
   ],
   "source": [
    "datafile = 'Data/FluxNormalisedDouble/data_lc.npy'\n",
    "labelsfile = 'Data/FluxNormalisedDouble/labels.npy'\n",
    "\n",
    "data = np.load(datafile)\n",
    "labels = np.load(labelsfile)\n",
    "\n",
    "print('Data Dimensions :', data.shape)\n",
    "print('Label Dimensions :', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2297f",
   "metadata": {},
   "source": [
    "## Prepare for Training\n",
    "### Output Information\n",
    "Filenames are ordered as follows:\n",
    "\n",
    "**ModelName_Modes_HU%%IT##**\n",
    "\n",
    "ModelName : Information about the NN structure\n",
    "- GRU : Gated Recurrent Unit\n",
    "- GRUbi : Bi-directional Gated Recurrent Unit\n",
    "- GRU2 : Two Gated Recurrent Units \n",
    "\n",
    "Modes : Information about training conditions\n",
    "- Pen : Class penalty applied for under-represented classes\n",
    "\n",
    "HU : Number of hidden units (given by %%)\n",
    "\n",
    "IT : Iteration number of given model (given by ##)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce328f",
   "metadata": {},
   "source": [
    "### Partitioning and One-Hot Encoding \n",
    "We ignore the TDEs for the moment. These events are associated with a -1 label. \n",
    "\n",
    "We keep 10% of the entire data set as a test set to evaluate model generalisation after training. \n",
    "\n",
    "One-hot encoding is performed for the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8c9884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels : [0 1 2 3 4 5]\n",
      "Training Label Proportions :  [ 115  705 2670   67  179  220]\n",
      "Test Label Proportions :  [ 14  87 301   6  13  19]\n",
      "Estimated Class Weights :  [23, 3, 1, 39, 14, 12] \n",
      "\n",
      "Training Data Dimensions : torch.Size([3956, 200, 2])\n",
      "Training Label Dimensions : torch.Size([3956, 6]) \n",
      "\n",
      "Test Data Dimensions : torch.Size([440, 200, 2])\n",
      "Test Label Dimensions : torch.Size([440, 6]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardise Data\n",
    "#data = StandardScaler().fit_transform(data)\n",
    "y = np.array([int(labels[i,1]) for i in range(labels.shape[0])])\n",
    "\n",
    "# Filter out TDEs\n",
    "idx = np.where(y >= 0)[0]\n",
    "Y = torch.tensor(y[idx])\n",
    "X = torch.permute(torch.tensor(data[idx]), [0,2,1])\n",
    "\n",
    "# Check that the proportion of each class is roughly the same in the training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
    "W = [1/count for count in np.unique(Y_train, return_counts=True)[1]]\n",
    "W = W / min(W)\n",
    "W = [int(element) for element in W]\n",
    "\n",
    "print('Unique Labels :', np.unique(Y_train, return_counts=True)[0])\n",
    "print('Training Label Proportions : ', np.unique(Y_train, return_counts=True)[1])\n",
    "print('Test Label Proportions : ', np.unique(Y_test, return_counts=True)[1])\n",
    "print('Estimated Class Weights : ', W, '\\n')\n",
    "\n",
    "# One-Hot Encoding\n",
    "Y = label_encoding(Y).float()\n",
    "\n",
    "# Final partitioning of the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
    "\n",
    "print('Training Data Dimensions :', X_train.shape)\n",
    "print('Training Label Dimensions :', Y_train.shape, '\\n')\n",
    "\n",
    "print('Test Data Dimensions :', X_test.shape)\n",
    "print('Test Label Dimensions :', Y_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523af09",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "### Model Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ee10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_ENID(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(simple_ENID, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, bidirectional=False) \n",
    "        self.Dense = nn.Linear(hidden_dim, output_dim)\n",
    "        self.predict = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_, h_ = self.GRU(x)\n",
    "        x_ = self.Dense(x_[:,-1])\n",
    "        x_ = self.predict(x_)\n",
    "        \n",
    "        return x_\n",
    "    \n",
    "class bi_ENID(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(bi_ENID, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, bidirectional=True) \n",
    "        self.Dense = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.predict = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_, h_ = self.GRU(x)\n",
    "        x_ = self.Dense(x_[:,-1])\n",
    "        x_ = self.predict(x_)\n",
    "        \n",
    "        return x_\n",
    "    \n",
    "class rapid_ENID(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(rapid_ENID, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, bidirectional=False) \n",
    "        self.Dropout = nn.Dropout(p=0.2)\n",
    "        self.Dense = nn.Linear(hidden_dim, output_dim)\n",
    "        self.predict = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_, h_ = self.GRU(x)\n",
    "        x_ = self.Dropout(x_)\n",
    "        x_, h_ = self.GRU(x_)\n",
    "        x_ = self.Dropout(x_)\n",
    "        x_ = self.Dense(x_[:,-1])\n",
    "        x_ = self.predict(x_)\n",
    "        \n",
    "        return x_\n",
    "\n",
    "class sexy_ENID(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, conv_out1, conv_out2, kernel1, kernel2):\n",
    "        super(sexy_ENID, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.channels1 = conv_out1\n",
    "        self.channels2 = conv_out2\n",
    "        self.kernel1 = kernel1\n",
    "        self.kernel2 = kernel2\n",
    "        self.conv1_out = 200 - 2*(self.kernel1 - 1)\n",
    "        self.conv2_out = self.conv1_out - 2 * (self.kernel2 - 1)\n",
    "        \n",
    "        self.GRU = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, bidirectional=False) \n",
    "        \n",
    "        self.Conv1 = nn.Conv1d(in_channels=2, out_channels=conv_out1, kernel_size=kernel1)\n",
    "        self.Conv2 = nn.Conv1d(in_channels=conv_out1, out_channels=conv_out2, kernel_size=kernel2)\n",
    "        self.Dense1_conv = nn.Linear(190, hidden_dim)\n",
    "        #self.Dense2_conv = nn.Linear(out1_conv, out2_conv)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #self.DenseGRU = nn.Linear(hidden_dim, out_GRU)\n",
    "        \n",
    "        self.Dense = nn.Linear(128, output_dim)\n",
    "        self.Dropout = nn.Dropout(p=0.2)\n",
    "        self.predict = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_GRU, h_ = self.GRU(x)\n",
    "        x_GRU = self.Dropout(x_GRU)\n",
    "        #x_GRU = self.Dense(x_GRU[:,-1])\n",
    "        \n",
    "        x_ = torch.permute(x, (0,2,1))\n",
    "        x_conv = self.Conv1(x_)\n",
    "        x_conv = self.relu(x_conv)\n",
    "        x_conv = self.Conv2(x_conv)\n",
    "        x_conv = self.relu(x_conv)\n",
    "        x_conv = self.Dropout(x_conv)\n",
    "        x_conv = self.Dense1_conv(x_conv)\n",
    "        x_conv = self.Dropout(x_conv)\n",
    "        \n",
    "        x_ENID = torch.cat((x_GRU, x_conv), dim=2)\n",
    "        \n",
    "        x_ENID = self.Dense(x_ENID[:,-1])\n",
    "        x_ENID = self.predict(x_ENID)\n",
    "        \n",
    "        return x_ENID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c1b8",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "#### GRU + Convolution (Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4cda466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sexy_ENID(\n",
      "  (GRU): GRU(2, 64, batch_first=True)\n",
      "  (Conv1): Conv1d(2, 100, kernel_size=(9,), stride=(1,))\n",
      "  (Conv2): Conv1d(100, 200, kernel_size=(3,), stride=(1,))\n",
      "  (Dense1_conv): Linear(in_features=190, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (Dense): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (Dropout): Dropout(p=0.2, inplace=False)\n",
      "  (predict): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 64\n",
    "n_layers = 1\n",
    "conv_out1 = 100\n",
    "conv_out2 = 200\n",
    "kernel1=9\n",
    "kernel2=3\n",
    "\n",
    "Net = sexy_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers, conv_out1=conv_out1, conv_out2=conv_out2, kernel1=kernel1, kernel2=kernel2)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRUconv_Pen_HU64BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e83bb2",
   "metadata": {},
   "source": [
    "#### GRU + Convolution (Big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "conv_out1 = 100\n",
    "conv_out2 = 200\n",
    "kernel1=9\n",
    "kernel2=3\n",
    "\n",
    "Net = sexy_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers, conv_out1=conv_out1, conv_out2=conv_out2, kernel1=kernel1, kernel2=kernel2)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRUconv_Pen_HU128BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02749268",
   "metadata": {},
   "source": [
    "#### GRU Simple (Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 64\n",
    "n_layers = 1\n",
    "\n",
    "Net = simple_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRU_Pen_HU64BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652c53b",
   "metadata": {},
   "source": [
    "#### GRU Simple (Batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce9dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 64\n",
    "n_layers = 1\n",
    "\n",
    "Net = simple_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRU_Pen_HU64BZ128EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc5ae5",
   "metadata": {},
   "source": [
    "#### GRU Simple (Medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "\n",
    "Net = simple_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRU_Pen_HU128BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a060d13",
   "metadata": {},
   "source": [
    "#### GRU Simple (Large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4907835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "Net = simple_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRU_Pen_HU256BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d1e78",
   "metadata": {},
   "source": [
    "#### Rapid GRU (Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 64\n",
    "n_layers = 1\n",
    "\n",
    "Net = rapid_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRURapid_Pen_HU64BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c7ef8",
   "metadata": {},
   "source": [
    "#### Rapid GRU (Big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24851e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "\n",
    "Net = rapid_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRURapid_Pen_HU128BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24a40d",
   "metadata": {},
   "source": [
    "#### Bi-directional GRU (Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ab466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 64\n",
    "n_layers = 1\n",
    "\n",
    "Net = bi_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRUbi_Pen_HU64BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d05438",
   "metadata": {},
   "source": [
    "#### Bi-directional GRU (Big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe52184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "\n",
    "Net = bi_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "# Initialisation\n",
    "version_name = 'GRUbi_Pen_HU128BZ64EP200IT01'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print('\\nTraining time:', train_time)\n",
    "\n",
    "# Model Evaluation\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2bbf7f",
   "metadata": {},
   "source": [
    "## Back-up Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8afaf",
   "metadata": {},
   "source": [
    "input_dim = X_train.shape[2]\n",
    "num_classes = Y_train.shape[1]\n",
    "hidden_dim = 32\n",
    "n_layers = 1\n",
    "\n",
    "print('Number of Classes :', num_classes, '\\n')\n",
    "\n",
    "Net = simple_ENID(input_dim=input_dim, hidden_dim =hidden_dim, output_dim=num_classes, n_layers=n_layers)\n",
    "print(Net)\n",
    "\n",
    "version_name = 'LossTest'\n",
    "version_directory = 'Models/'+version_name\n",
    "\n",
    "if not os.path.exists(version_directory):\n",
    "    os.makedirs(version_directory)\n",
    "    \n",
    "batch_size = 32\n",
    "num_epochs = 4\n",
    "\n",
    "evaluate = nn.CrossEntropyLoss(weight=torch.tensor(W))\n",
    "\n",
    "start_time = time.time()\n",
    "trained_network, train_loss = network_train(Net, X_train, Y_train, batch_size, num_epochs, evaluate, version_directory)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "network_evaluation(trained_network, X_test, Y_test, train_loss, evaluate, version_directory, train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccdedcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_sample(data, labels, n):\n",
    "    num_samples = n\n",
    "    uniques = np.arange(len(np.unique(labels))-1)\n",
    "    sub_data = np.zeros((len(uniques)*num_samples, data.shape[1], data.shape[2]))\n",
    "    sub_labels = np.zeros((len(uniques)*num_samples))\n",
    "    \n",
    "    i = 0\n",
    "    for unique in uniques:\n",
    "        idx = np.where(labels == str(unique))[0]\n",
    "        sub_data[i:i+num_samples,:] = data[idx[0:num_samples],:,:]\n",
    "        sub_labels[i:i+num_samples] = labels[idx[0:num_samples]]\n",
    "        i += num_samples\n",
    "        \n",
    "    return sub_data, sub_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
